{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.5 huggingface-hub-0.33.4 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.2 tqdm-4.67.1 transformers-4.53.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Installing collected packages: click, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [nltk][32m1/2\u001b[0m [nltk]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.2.1 nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cO_mCsQqov8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import pickle\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3687,
     "status": "ok",
     "timestamp": 1726927937708,
     "user": {
      "displayName": "Review Paper",
      "userId": "09140924793698071071"
     },
     "user_tz": -330
    },
    "id": "srVGFyN6MEG1",
    "outputId": "c7ac8731-47b1-40e7-938b-86f85dcb129f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg-b6DVSn_Z2"
   },
   "source": [
    "### Load TTPXHunter from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3463,
     "status": "ok",
     "timestamp": 1726927941169,
     "user": {
      "displayName": "Review Paper",
      "userId": "09140924793698071071"
     },
     "user_tz": -330
    },
    "id": "Kf7xE9b2nsh5",
    "outputId": "82d15ebf-1433-4bf3-974f-6180b3e79b42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=193, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"nanda-rani/TTPXHunter\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"nanda-rani/TTPXHunter\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SFX5RzAQ5B1z"
   },
   "outputs": [],
   "source": [
    "def extract_ttp_from_sentences(sentences, threshold, label_dict, ttpid2name):\n",
    "    \"\"\"\n",
    "    Extract TTP (Tactics, Techniques, and Procedures) based on a prediction threshold from the given sentences.\n",
    "\n",
    "    Args:\n",
    "    - sentences (list of str): List of sentences to extract TTP from.\n",
    "    - threshold (float): Confidence threshold for accepting predictions.\n",
    "\n",
    "    Returns:\n",
    "    - unique_ttp_ids (list of int): Unique TTP IDs extracted from the sentences.\n",
    "    - names_for_ttp_ids (list of str): Human-readable names corresponding to the TTP IDs.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    # Loop over sentences and perform inference\n",
    "    for text in sentences:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Perform inference without gradient tracking\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits and compute probabilities\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        max_prob, predicted_class_indices = torch.max(probabilities, dim=1)\n",
    "\n",
    "        # Filter predictions based on the confidence threshold\n",
    "        predicted_labels = [\n",
    "            model.config.id2label[class_idx.item()]\n",
    "            for prob, class_idx in zip(max_prob, predicted_class_indices)\n",
    "            if prob.item() > threshold\n",
    "        ]\n",
    "\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "    # Map the predicted labels to integer labels\n",
    "    mapped_labels = [int(label.split('_')[1]) for label in predictions]\n",
    "\n",
    "    # Load the label-to-name dictionary\n",
    "    with open(label_dict, 'rb') as file:\n",
    "        label_dict = pickle.load(file)\n",
    "\n",
    "    # Invert the dictionary to map integer labels to TTP names\n",
    "    inverted_label_dict = {v: k for k, v in label_dict.items()}\n",
    "    ttp_list = [inverted_label_dict[label] for label in mapped_labels]\n",
    "\n",
    "    # Extract unique TTP IDs\n",
    "    unique_ttp_ids = list(set(ttp_list))\n",
    "\n",
    "    # Translate TTP IDs to their names\n",
    "    names_for_ttp_ids = translate_ttp_ids_to_names(unique_ttp_ids, ttpid2name)\n",
    "\n",
    "    return unique_ttp_ids, names_for_ttp_ids\n",
    "\n",
    "def remove_consecutive_newlines(text):\n",
    "    \"\"\"\n",
    "    Remove consecutive newlines from a string.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input string with potential consecutive newlines.\n",
    "\n",
    "    Returns:\n",
    "    - str: String with consecutive newlines reduced to single newlines.\n",
    "    \"\"\"\n",
    "    cleaned_text = text[0]\n",
    "    for char in text[1:]:\n",
    "        if not (char == cleaned_text[-1] and cleaned_text[-1] == '\\n'):\n",
    "            cleaned_text += char\n",
    "    return cleaned_text\n",
    "\n",
    "def process_text_file_for_attack_patterns(file_name, threshold, label_dict, ttpid2name):\n",
    "    \"\"\"\n",
    "    Read and process a text file to extract attack patterns using TTP extraction.\n",
    "\n",
    "    Args:\n",
    "    - file_name (str): Path to the input text file.\n",
    "    - threshold (float): Confidence threshold for TTP extraction.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (unique TTP IDs, names corresponding to TTP IDs).\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    # Read the text file\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Clean the text by removing consecutive newlines and tabs\n",
    "    text = remove_consecutive_newlines(text)\n",
    "    text = text.replace('\\t', ' ').replace(\"\\'\", \"'\")\n",
    "\n",
    "    # Tokenize sentences\n",
    "    tokenized_sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Split tokenized sentences by newlines and filter empty lines\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentences += [line for line in sentence.split('\\n') if len(line) > 0]\n",
    "\n",
    "    # Extract TTP from the processed sentences\n",
    "    return extract_ttp_from_sentences(sentences, threshold, label_dict, ttpid2name)\n",
    "\n",
    "def translate_ttp_ids_to_names(ttp_ids, ttpid2name):\n",
    "    \"\"\"\n",
    "    Translate TTP (Tactics, Techniques, and Procedures) IDs to human-readable names.\n",
    "\n",
    "    Args:\n",
    "    - ttp_ids (list of int): List of TTP IDs to translate.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: Corresponding human-readable names for the TTP IDs.\n",
    "    \"\"\"\n",
    "    # Load the TTP ID to name mapping from a file\n",
    "    with open(ttpid2name, 'rb') as file:\n",
    "        id_to_name_map = pickle.load(file)\n",
    "\n",
    "    # Translate each TTP ID to its corresponding name\n",
    "    ttp_names = [id_to_name_map[ttp_id] for ttp_id in ttp_ids if ttp_id in id_to_name_map]\n",
    "\n",
    "    return ttp_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21312,
     "status": "ok",
     "timestamp": 1726927962479,
     "user": {
      "displayName": "Review Paper",
      "userId": "09140924793698071071"
     },
     "user_tz": -330
    },
    "id": "OuDw4osy5RHL",
    "outputId": "65d9b483-3086-495c-d011-4e1cc224be75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "T1195  -  Supply Chain Compromise\n",
      "T1587  -  Develop Capabilities\n"
     ]
    }
   ],
   "source": [
    "label_dict = 'label_dict.pkl'\n",
    "ttpid2name = 'ttp_id_name.pkl'\n",
    "report = \"SharpPanda_APT_Campaign_Expands_its_Arsenal_Targeting_G20_Nations.txt\"\n",
    "th = 0.644\n",
    "\n",
    "ttps, ttp_names = process_text_file_for_attack_patterns(report, th, label_dict, ttpid2name)\n",
    "print(len(ttps))\n",
    "\n",
    "for i in range(len(ttps)):\n",
    "  print(ttps[i], \" - \", ttp_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zc1q-gWv7HKJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhYOjCnGt608EQ+YfW9knU",
   "mount_file_id": "1JM-tfdiUqKIClHdfY4CKjGSludNwVv-B",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
